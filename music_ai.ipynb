{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "import keras.backend as K\n",
    "from keras import metrics\n",
    "from keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/hibat/Desktop/Github/Music_ai/data/\"\n",
    "files = [i for i in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset and create list of notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "\n",
    "  #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "  #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "            notes_to_parse = part.recurse() \n",
    "      #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                print(element)\n",
    "                if isinstance(element, note.Note):\n",
    "                      notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                      notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "      \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_notes=[]\n",
    "for i in files:\n",
    "    all_notes.append(read_midi(os.path.join(path, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = [element for notes in all_notes for element in notes]\n",
    "len(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of a input sequence\n",
    "no_of_timesteps = 128      \n",
    "\n",
    "#no. of unique notes\n",
    "n_vocab = len(set(notes))  \n",
    "print(n_vocab)\n",
    "#all the unique notes\n",
    "pitch = sorted(set(item for item in notes))  \n",
    "print(pitch)\n",
    "#assign unique value to every note\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitch))\n",
    "print(len(note_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing input and output sequences\n",
    "X = []\n",
    "y = []\n",
    "for notes in all_notes:\n",
    "    for i in range(0, len(notes) - no_of_timesteps, 1):\n",
    "        input_ = notes[i:i + no_of_timesteps]\n",
    "        #print(input_)\n",
    "        #print(len(input_))\n",
    "        output = notes[i + no_of_timesteps]\n",
    "        #print(output)\n",
    "        X.append([note_to_int[note] for note in input_])\n",
    "        y.append(note_to_int[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.reshape(X, (len(X), no_of_timesteps, 1))\n",
    "#normalizing the inputs\n",
    "X = X / float(n_vocab)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![title](LSTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(n_vocab))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',  metrics=['mae', 'acc'])\n",
    "mc = callbacks.ModelCheckpoint('weights{epoch:08d}.h5', \n",
    "                                     save_weights_only=True, period=20)\n",
    "model.fit(X,np.array(y), epochs=100, batch_size=128)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('music_ai.h5') # save weights of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert output of the model into notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, pitch, no_of_timesteps, pattern):\n",
    "    \n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitch))\n",
    "    \n",
    "    \n",
    "    prediction_output = []\n",
    "    \n",
    "    # generate 50 elements\n",
    "    for note_index in range(50):\n",
    "        #reshaping array to feed into model\n",
    "        Input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        \n",
    "        #predict the probability and choose the maximum value\n",
    "        proba = model.predict(Input)\n",
    "        \n",
    "        index = np.argmax(proba)\n",
    "        \n",
    "        #convert integer back to the element\n",
    "        if index > len(int_to_note):\n",
    "            pass\n",
    "        else:\n",
    "            pred = int_to_note[index]\n",
    "            prediction_output.append(pred)\n",
    "            \n",
    "            pattern = list(pattern)\n",
    "            pattern.append(index/float(n_vocab))\n",
    "            #leave the first value at index 0\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert notes into midi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    print(output_notes)\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # Specify duration between 2 notes\n",
    "        offset+= 0.5\n",
    "       # offset += random.uniform(0.5,0.9)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    midi_stream.write('midi', fp='music_2.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = np.random.randint(0, len(X)-1)\n",
    "pattern = X[start]\n",
    "\n",
    "#load the best model\n",
    "model=load_model('music_ai.h5')\n",
    "#generate and save music\n",
    "music = generate_music(model,pitch,no_of_timesteps,pattern)\n",
    "convert_to_midi(music)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
